# ğŸ¤Ÿ Hand Sign Detection System

A Kivy-based mobile/desktop application that performs **real-time hand sign detection** and converts detected signs into text and speech.

![Platform](https://img.shields.io/badge/Platform-Android%20%7C%20Windows%20%7C%20Linux-blue)
![Python](https://img.shields.io/badge/Python-3.8%2B-green)
![Kivy](https://img.shields.io/badge/Kivy-2.2%2B-orange)

---

## âœ¨ Features

- ğŸ“¹ **Live Camera Feed** - Real-time camera processing
- ğŸ–ï¸ **Gesture Detection** - Recognizes 4 hand signs (Open, Close, Pointer, OK)
- ğŸ“ **Text Conversion** - Converts gestures to readable text
- ğŸ”Š **Text-to-Speech** - Speaks detected gestures aloud
- ğŸ“± **Android Compatible** - Build APK for Android devices
- ğŸ–¥ï¸ **Desktop Support** - Works on Windows, Linux, macOS

---

## ğŸ¯ Supported Gestures

| Gesture     | Sign | Text Output |
| ----------- | ---- | ----------- |
| Open Palm   | ğŸ‘‹   | Hello       |
| Closed Fist | âœŠ   | Yes         |
| Pointing    | ğŸ‘†   | Look        |
| OK Sign     | ğŸ‘Œ   | OK          |

---

## ğŸ“ Project Structure

```
HSL/
â”œâ”€â”€ android_app.py          # ğŸ“± Android-compatible main app
â”œâ”€â”€ main.py                 # ğŸ–¥ï¸ Desktop app (with MediaPipe)
â”œâ”€â”€ main.kv                 # UI layout (Kivy language)
â”œâ”€â”€ buildozer.spec          # Android build configuration
â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚
â”œâ”€â”€ gesture_model.tflite    # TensorFlow Lite model (full)
â”œâ”€â”€ gesture_model_quant.tflite # TensorFlow Lite model (quantized)
â”œâ”€â”€ gesture_labels.txt      # Model label mapping
â”‚
â”œâ”€â”€ camera/                 # Camera handling module
â”‚   â””â”€â”€ camera_stream.py
â”œâ”€â”€ detection/              # Detection logic
â”‚   â”œâ”€â”€ hand_detector.py    # MediaPipe hand detection
â”‚   â””â”€â”€ gesture_logic.py    # Gesture classification
â”œâ”€â”€ services/               # Services
â”‚   â”œâ”€â”€ tts.py             # Text-to-speech
â”‚   â””â”€â”€ translator.py      # Translation (future)
â””â”€â”€ utils/                  # Utilities
    â””â”€â”€ constants.py        # App constants
```

---

## ğŸš€ Quick Start

### Desktop (Windows/Linux/macOS)

1. **Clone the repository**

   ```bash
   git clone <repository-url>
   cd HSL
   ```

2. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

3. **Run the desktop app**

   ```bash
   # With MediaPipe (full accuracy)
   python main.py

   # Android-compatible version (TFLite)
   python android_app.py
   ```

---

## ğŸ“± Building Android APK

### Prerequisites

- **Linux** or **Windows with WSL** (Ubuntu recommended)
- Python 3.8+
- Android SDK & NDK (auto-installed by Buildozer)

### Step-by-Step Guide

#### 1. Install WSL (Windows only)

```powershell
# Run in PowerShell as Administrator
wsl --install -d Ubuntu
```

#### 2. Set up build environment (in WSL/Linux)

```bash
# Update system
sudo apt update && sudo apt upgrade -y

# Install dependencies
sudo apt install -y python3-pip python3-venv git zip unzip \
    openjdk-17-jdk autoconf libtool pkg-config \
    zlib1g-dev libncurses5-dev libncursesw5-dev libtinfo5 \
    cmake libffi-dev libssl-dev

# Install Buildozer and Cython
pip3 install --upgrade buildozer cython virtualenv
```

#### 3. Navigate to project directory

```bash
cd /mnt/g/collegeProject/HSL  # Adjust path as needed
```

#### 4. Build the APK

```bash
# Debug build (for testing)
buildozer android debug

# The APK will be in: bin/handsigndetection-1.0.0-arm64-v8a-debug.apk
```

#### 5. Install on Android device

```bash
# Connect your Android device and enable USB debugging
buildozer android deploy run logcat
```

---

## ğŸ”§ Configuration

### Buildozer Settings (`buildozer.spec`)

| Setting         | Description                                    |
| --------------- | ---------------------------------------------- |
| `title`         | App name shown on device                       |
| `package.name`  | Unique identifier                              |
| `android.archs` | CPU architectures (`arm64-v8a`, `armeabi-v7a`) |
| `android.api`   | Target Android API level                       |
| `requirements`  | Python packages to include                     |

### Modifying Gestures

Edit `utils/constants.py` to change gesture mappings:

```python
GESTURE_MAP = {
    "Open": "Hello",    # Change the text output
    "Close": "Yes",
    "Pointer": "Look",
    "OK": "OK",
}
```

---

## ğŸ§  Model Training

To train a new gesture model:

1. **Collect data**

   ```bash
   python data_collector.py
   ```

2. **Download additional dataset**

   ```bash
   python download_dataset.py
   ```

3. **Train TFLite model**
   ```bash
   python train_tflite.py
   ```

This generates:

- `gesture_model.tflite` - Full precision model
- `gesture_model_quant.tflite` - Quantized (smaller) model
- `gesture_labels.txt` - Label mapping

---

## ğŸ› ï¸ Troubleshooting

### Build Errors

| Error                      | Solution                                            |
| -------------------------- | --------------------------------------------------- |
| `SDK license not accepted` | Run with `android.accept_sdk_license = True`        |
| `NDK not found`            | Let Buildozer auto-install or set `ANDROIDSDK` path |
| `Recipe failed`            | Clean build: `buildozer android clean`              |

### Runtime Errors

| Error              | Solution                                      |
| ------------------ | --------------------------------------------- |
| Camera not working | Check `CAMERA` permission in Android settings |
| TTS not speaking   | Enable TTS engine in Android settings         |
| Model not loading  | Ensure `.tflite` files are in project root    |

---

## ğŸ“„ License

This project is for educational purposes (College Project).

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Open a pull request

---

## ğŸ“ Support

For issues or questions, please open a GitHub issue.
