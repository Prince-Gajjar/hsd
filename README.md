<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/Kivy-2.2+-5E5E5E?style=for-the-badge&logo=kivy&logoColor=white" alt="Kivy"/>
  <img src="https://img.shields.io/badge/TensorFlow_Lite-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white" alt="TensorFlow Lite"/>
  <img src="https://img.shields.io/badge/MediaPipe-0097A7?style=for-the-badge&logo=google&logoColor=white" alt="MediaPipe"/>
</p>

<h1 align="center">ğŸ¤Ÿ Hand Sign Detection System</h1>

<p align="center">
  <strong>Real-time hand gesture recognition with Text-to-Speech conversion</strong>
</p>

<p align="center">
  A cross-platform application that uses computer vision and machine learning to detect hand signs in real-time, convert them to text, and speak them aloud using text-to-speech technology.
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Platform-Windows%20|%20Linux%20|%20macOS%20|%20Android-blue?style=flat-square" alt="Platforms"/>
  <img src="https://img.shields.io/badge/License-Educational-green?style=flat-square" alt="License"/>
  <img src="https://img.shields.io/badge/Status-Active-success?style=flat-square" alt="Status"/>
</p>

---

## ğŸ“‹ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸ¯ Supported Gestures](#-supported-gestures)
- [ğŸ› ï¸ Technology Stack](#ï¸-technology-stack)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸš€ Getting Started](#-getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
  - [Running the Application](#running-the-application)
- [ğŸ“± Android APK Build](#-android-apk-build)
- [ğŸ§  Model Training](#-model-training)
- [âš™ï¸ Configuration](#ï¸-configuration)
- [ğŸ”§ Troubleshooting](#-troubleshooting)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ¥ Real-time Detection

- Live camera feed processing at 15-30 FPS
- MediaPipe-powered hand landmark detection
- Visual landmark overlay on detected hands

</td>
<td width="50%">

### ğŸ§  Smart Recognition

- TensorFlow Lite model for gesture classification
- Rule-based fallback detection
- 60%+ confidence threshold for accuracy

</td>
</tr>
<tr>
<td width="50%">

### ğŸ”Š Text-to-Speech

- Convert gestures to spoken words
- Cross-platform TTS support
- Native Android TTS integration

</td>
<td width="50%">

### ğŸ“± Cross-Platform

- Desktop: Windows, Linux, macOS
- Mobile: Android APK support
- Consistent UI across platforms

</td>
</tr>
</table>

---

## ğŸ¯ Supported Gestures

The system currently recognizes **4 hand gestures**:

|     Gesture     | Visual | Recognized As | Speech Output |
| :-------------: | :----: | :-----------: | :-----------: |
|  **Open Palm**  |   ğŸ‘‹   |    `Open`     |    "Hello"    |
| **Closed Fist** |   âœŠ   |    `Close`    |     "Yes"     |
|  **Pointing**   |   ğŸ‘†   |   `Pointer`   |    "Look"     |
|   **OK Sign**   |   ğŸ‘Œ   |     `OK`      |     "OK"      |

---

## ğŸ› ï¸ Technology Stack

| Component            | Technology      | Purpose                            |
| -------------------- | --------------- | ---------------------------------- |
| **UI Framework**     | Kivy 2.2+       | Cross-platform user interface      |
| **Hand Detection**   | MediaPipe       | Hand landmark detection (Desktop)  |
| **ML Model**         | TensorFlow Lite | Gesture classification             |
| **Desktop TTS**      | pyttsx3         | Text-to-speech (Windows/Linux/Mac) |
| **Mobile TTS**       | Plyer           | Text-to-speech (Android)           |
| **Image Processing** | OpenCV, NumPy   | Camera and frame processing        |
| **Build Tool**       | Buildozer       | Android APK generation             |

---

## ğŸ“ Project Structure

```
hsd/
â”œâ”€â”€ ğŸ“„ main.py                  # Desktop application (MediaPipe)
â”œâ”€â”€ ğŸ“„ android_app.py           # Android-compatible app (TFLite)
â”œâ”€â”€ ğŸ“„ main.kv                  # Kivy UI layout file
â”œâ”€â”€ ğŸ“„ buildozer.spec           # Android build configuration
â”œâ”€â”€ ğŸ“„ requirements.txt         # Python dependencies
â”‚
â”œâ”€â”€ ğŸ§  Models & Data
â”‚   â”œâ”€â”€ gesture_model.tflite        # Full precision TFLite model
â”‚   â”œâ”€â”€ gesture_model_quant.tflite  # Quantized model (smaller)
â”‚   â”œâ”€â”€ gesture_model.pickle        # Trained model (pickle format)
â”‚   â”œâ”€â”€ gesture_labels.txt          # Label mapping file
â”‚   â”œâ”€â”€ keypoint.csv                # Training data
â”‚   â””â”€â”€ keypoint_labels.csv         # Label definitions
â”‚
â”œâ”€â”€ ğŸ“ camera/                  # Camera handling module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ camera_stream.py        # Camera capture and processing
â”‚
â”œâ”€â”€ ğŸ“ detection/               # Detection logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ hand_detector.py        # MediaPipe hand detection
â”‚   â””â”€â”€ gesture_logic.py        # Gesture classification rules
â”‚
â”œâ”€â”€ ğŸ“ services/                # Application services
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tts.py                  # Text-to-speech service
â”‚   â””â”€â”€ translator.py           # Translation service (future)
â”‚
â”œâ”€â”€ ğŸ“ utils/                   # Utility modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ constants.py            # App constants and config
â”‚
â”œâ”€â”€ ğŸ› ï¸ Training Scripts
â”‚   â”œâ”€â”€ data_collector.py       # Collect training data
â”‚   â”œâ”€â”€ download_dataset.py     # Download external datasets
â”‚   â”œâ”€â”€ create_model.py         # Create and export model
â”‚   â”œâ”€â”€ train_model.py          # Train classifier
â”‚   â””â”€â”€ train_tflite.py         # Convert to TFLite format
â”‚
â””â”€â”€ ğŸ“ assets/                  # App assets (icons, images)
```

---

## ğŸš€ Getting Started

### Prerequisites

| Requirement | Version    | Notes                            |
| ----------- | ---------- | -------------------------------- |
| **Python**  | 3.8 - 3.11 | Python 3.12+ not fully supported |
| **pip**     | Latest     | Package manager                  |
| **Webcam**  | Any        | For hand detection               |
| **Git**     | Any        | For cloning repository           |

### Installation

#### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Prince-Gajjar/hsd.git
cd hsd
```

#### 2ï¸âƒ£ Create Virtual Environment (Recommended)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/macOS
python3 -m venv venv
source venv/bin/activate
```

#### 3ï¸âƒ£ Install Dependencies

```bash
# Install core dependencies
pip install -r requirements.txt

# Install additional desktop dependencies
pip install opencv-python mediapipe tensorflow
```

### Running the Application

#### ğŸ–¥ï¸ Desktop Version (Full Features)

The desktop version uses **MediaPipe** for accurate hand detection:

```bash
python main.py
```

#### ğŸ“± Android-Compatible Version

For testing the Android-compatible version on desktop:

```bash
python android_app.py
```

---

## ğŸ“± Android APK Build

### Requirements

- **Linux** or **Windows with WSL** (Ubuntu 20.04+ recommended)
- **Buildozer** and **Cython**
- **Android SDK/NDK** (auto-installed by Buildozer)

### Step-by-Step Build Guide

#### 1ï¸âƒ£ Set Up WSL (Windows Users)

```powershell
# Run in PowerShell as Administrator
wsl --install -d Ubuntu
```

#### 2ï¸âƒ£ Install Build Dependencies (WSL/Linux)

```bash
# Update system
sudo apt update && sudo apt upgrade -y

# Install required packages
sudo apt install -y \
    python3-pip python3-venv git zip unzip \
    openjdk-17-jdk autoconf libtool pkg-config \
    zlib1g-dev libncurses5-dev libncursesw5-dev libtinfo5 \
    cmake libffi-dev libssl-dev

# Install Buildozer and Cython
pip3 install --upgrade buildozer cython virtualenv
```

#### 3ï¸âƒ£ Navigate to Project Directory

```bash
# Adjust the path based on your Windows drive
cd /mnt/g/collegeProject/hsd
```

#### 4ï¸âƒ£ Build the APK

```bash
# Debug build (for testing)
buildozer android debug

# The APK will be generated at:
# bin/handsigndetection-1.0.0-arm64-v8a-debug.apk
```

#### 5ï¸âƒ£ Install on Android Device

```bash
# Connect your device via USB (enable USB debugging)
buildozer android deploy run logcat
```

> ğŸ’¡ **Tip:** First build may take 20-40 minutes as it downloads Android SDK/NDK.

---

## ğŸ§  Model Training

Train your own gesture recognition model:

### Step 1: Collect Training Data

```bash
python data_collector.py
```

- Shows camera preview
- Press keys (0-3) to record gestures
- Data saved to `keypoint.csv`

### Step 2: Download Additional Dataset (Optional)

```bash
python download_dataset.py
```

### Step 3: Train the Model

```bash
# Train and create pickle model
python train_model.py

# Convert to TFLite format
python train_tflite.py
```

### Output Files

| File                         | Description        | Size    |
| ---------------------------- | ------------------ | ------- |
| `gesture_model.pickle`       | Full trained model | ~2.7 MB |
| `gesture_model.tflite`       | TFLite model       | ~24 KB  |
| `gesture_model_quant.tflite` | Quantized TFLite   | ~14 KB  |
| `gesture_labels.txt`         | Label mapping      | <1 KB   |

---

## âš™ï¸ Configuration

### Gesture Mappings

Edit `utils/constants.py` to customize gestures:

```python
# Gesture to Text Mapping
GESTURE_MAP = {
    "Open": "Hello",      # Open palm â†’ "Hello"
    "Close": "Yes",       # Closed fist â†’ "Yes"
    "Pointer": "Look",    # Pointing â†’ "Look"
    "OK": "OK",           # OK sign â†’ "OK"
}

# Add custom gestures here
```

### Camera Settings

```python
# In utils/constants.py
CAMERA_RESOLUTION = (640, 480)  # Camera resolution
CAMERA_INDEX = 0                 # Camera device index
DETECTION_FPS = 15               # Detection rate
```

### Model Settings

```python
# In utils/constants.py
CONFIDENCE_THRESHOLD = 0.6       # Min confidence (0-1)
GESTURE_DEBOUNCE_TIME = 1.5      # Seconds between detections
```

---

## ğŸ”§ Troubleshooting

### Common Issues & Solutions

<details>
<summary><b>âŒ Camera not opening</b></summary>

**Solution:**

1. Check if another app is using the camera
2. Try different camera index: Change `CAMERA_INDEX = 1` in `constants.py`
3. On Linux: Run `sudo chmod 666 /dev/video0`

</details>

<details>
<summary><b>âŒ MediaPipe import error</b></summary>

**Solution:**

```bash
pip uninstall mediapipe
pip install mediapipe==0.10.0
```

</details>

<details>
<summary><b>âŒ TTS not speaking (Windows)</b></summary>

**Solution:**

1. Install/reinstall pyttsx3: `pip install --upgrade pyttsx3`
2. Check Windows speech settings
3. Try: `pip install pywin32`

</details>

<details>
<summary><b>âŒ Buildozer build fails</b></summary>

**Solution:**

1. Clean build: `buildozer android clean`
2. Update Buildozer: `pip install --upgrade buildozer`
3. Check Java version: `java -version` (need JDK 17)
4. Accept SDK licenses: Add `android.accept_sdk_license = True` to `buildozer.spec`

</details>

<details>
<summary><b>âŒ Model not loading</b></summary>

**Solution:**

1. Ensure `.tflite` files are in project root
2. Check file permissions
3. Verify TensorFlow is installed: `pip install tensorflow`

</details>

### Build Errors Quick Reference

| Error                      | Solution                                                    |
| -------------------------- | ----------------------------------------------------------- |
| `SDK license not accepted` | Add `android.accept_sdk_license = True` to `buildozer.spec` |
| `NDK not found`            | Let Buildozer auto-install or set path manually             |
| `Recipe failed`            | Run `buildozer android clean` and rebuild                   |
| `AAPT2 error`              | Delete `.buildozer` folder and rebuild                      |

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature/amazing-feature`
3. **Commit** your changes: `git commit -m 'Add amazing feature'`
4. **Push** to the branch: `git push origin feature/amazing-feature`
5. **Open** a Pull Request

### Ideas for Contribution

- [ ] Add more gesture types
- [ ] Improve hand detection accuracy
- [ ] Add multi-language TTS support
- [ ] Create iOS version
- [ ] Add gesture recording/playback

---

## ğŸ“„ License

This project is developed for **educational purposes** as a college project.

---

<p align="center">
  <strong>Made with â¤ï¸ for accessibility</strong>
</p>

<p align="center">
  <a href="#-hand-sign-detection-system">â¬†ï¸ Back to Top</a>
</p>
